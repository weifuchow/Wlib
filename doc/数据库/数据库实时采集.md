# 数据库实时采集

## 概述

- 所谓的实时采集目的就是要CDC(change data **capture**)捕获数据变化。
- 各个数据库厂商所提供的CDC方式并没有标准。所以很难有统一的方案。
- 常见的CDC方案有：
  - 数据库提供的逻辑日志记录，
  - 利用数据库的触发器实现。



## Mysql

- 基于binlog 进行的日志记录。

- 模拟mysql主从之间同步操作。

- 配置：

  - 开启binlog 日志

- 流程如下：

  - 使用binlog client 客户端与mysql 监听库进行通信。

    - keep alive (心跳保活)

      - ```java
        private void enableHeartbeat() throws IOException {
            channel.write(new QueryCommand("set @master_heartbeat_period=" + heartbeatInterval * 1000000));
            byte[] statementResult = channel.read();
            checkError(statementResult);
        }
        ```

    - binlog 采用 bio 的信息访问。单客户端线程

  - 发出binlog 日志信息查询。

    - 只是初始化时发送一次，后续server收到之后就会直接推送给client  ，event信息。

    - ```java
      private void requestBinaryLogStream() throws IOException {
          long serverId = blocking ? this.serverId : 0; // http://bugs.mysql.com/bug.php?id=71178
          Command dumpBinaryLogCommand;
          synchronized (gtidSetAccessLock) {
              if (gtidSet != null) {
                  dumpBinaryLogCommand = new DumpBinaryLogGtidCommand(serverId,
                                                                      useBinlogFilenamePositionInGtidMode ? binlogFilename : "",
                                                                      useBinlogFilenamePositionInGtidMode ? binlogPosition : 4,
                                                                      gtidSet);
              } else {
                  dumpBinaryLogCommand = new DumpBinaryLogCommand(serverId, binlogFilename, binlogPosition);
              }
          }
          channel.write(dumpBinaryLogCommand);
      }
      ```

  - 注册响应监听器，将返回信息进行解码(转化为Event)然后获取对应的sql.

    - 死循环监听inputsteam,序列化成event.

    - 然后通知注册的listener 进行出力 。notifyEventListeners 

    - ```java
      private void listenForEventPackets() throws IOException {
          ByteArrayInputStream inputStream = channel.getInputStream();
          boolean completeShutdown = false;
          try {
              while (inputStream.peek() != -1) {
                  int packetLength = inputStream.readInteger(3);
                  inputStream.skip(1); // 1 byte for sequence
                  int marker = inputStream.read();
                  if (marker == 0xFF) {
                      ErrorPacket errorPacket = new ErrorPacket(inputStream.read(packetLength - 1));
                      throw new ServerException(errorPacket.getErrorMessage(), errorPacket.getErrorCode(),
                                                errorPacket.getSqlState());
                  }
                  if (marker == 0xFE && !blocking) {
                      completeShutdown = true;
                      break;
                  }
                  Event event;
                  try {
                      event = eventDeserializer.nextEvent(packetLength == MAX_PACKET_LENGTH ?
                                                          new ByteArrayInputStream(readPacketSplitInChunks(inputStream, packetLength - 1)) :
                                                          inputStream);
                      if (event == null) {
                          throw new EOFException();
                      }
                  } catch (Exception e) {
                      Throwable cause = e instanceof EventDataDeserializationException ? e.getCause() : e;
                      if (cause instanceof EOFException || cause instanceof SocketException) {
                          throw e;
                      }
                      if (isConnected()) {
                          for (LifecycleListener lifecycleListener : lifecycleListeners) {
                              lifecycleListener.onEventDeserializationFailure(this, e);
                          }
                      }
                      continue;
                  }
                  if (isConnected()) {
                      eventLastSeen = System.currentTimeMillis();
                      updateGtidSet(event);
                      notifyEventListeners(event);
                      updateClientBinlogFilenameAndPosition(event);
                  }
              }
          } catch (Exception e) {
              if (isConnected()) {
                  for (LifecycleListener lifecycleListener : lifecycleListeners) {
                      lifecycleListener.onCommunicationFailure(this, e);
                  }
              }
          } finally {
              if (isConnected()) {
                  if (completeShutdown) {
                      disconnect(); // initiate complete shutdown sequence (which includes keep alive thread)
                  } else {
                      disconnectChannel();
                  }
              }
          }
      }
      ```

      

- 注意：
  - binlog 也有几种模式。这里在mark 后续再补充

## Oracle

- 使用Oracle公司提供的LOGMNR工具，加载在线redo log，然后通过jdbc的方式，查询出让数据改变的sql，然后解析sql，
- 在Oracle数据库中，LOGMNR进程将数据库中进行的DML等操作信息记录在日志文件中，在归档模式下，日志文件还会写出到归档日志文件中。在数据库发生故障崩溃后，恢复时数据库可以根据日志信息来重演事务，完成恢复，从而保证成功提交的事务不丢失。
      Oracle随数据库软件提供了一个LOGMNR工具，可以很容易实现对于日志的解析，熟悉LOGMNR的使用在很多时候可以帮助我们分析数据库问题，找出根本原因。
      利用logmnr，可以做以下事情
      1、查明数据库的变更记录，或者是进行变化分析，如有的用户怀疑自己的应用有问题，产生了大量的失误，可以用logmnr来分析这些事务，看看到底发生了些什么事情。
      2、侦察并更正用户的误操作，如有的用户一不小心误删除了某个表，但是并不承认，这个时候就可利用logmnr来分析谁执行的DML或者是DDL操作。
      3、找回失去的数据，当不能使用flashback或使用flashback受限的时候，我们可以考虑利用logmnr来找回数据，这时候，只要有归档日志即可。

- 配置
  - 开启归档日志模式 （ARCHIVELOG）

- 使用介绍

  - > 1、longminer的环境配置
    >
    > https://www.cnblogs.com/shishanyuan/p/3140440.html
    >
    > 2、sqlplus中执行logminer的命令记录
    >
    > //login
    >
    > CONNECT DATAHUB/DATAHUB
    >
    >
    > //create table and insert data
    > CREATE TABLE "DATAHUB"."ORACDC"("ID" NUMBER(5),"NAME" VARCHAR2(10));   
    >
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (1, 'a');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (2, 'b');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (3, 'c');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (4, 'd');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (5, 'e');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (6, 'f');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (7, 'g');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (8, 'h');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (9, 'i');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (10, 'g');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (11, 'k');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (12, 'l');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (13, 'm');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (14, 'n');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (15, 'o');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (16, 'p');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (17, 'q');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (18, 'r');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (19, 's');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (20, 't');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (21, 'u');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (22, 'v');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (23, 'w');
    > INSERT INTO "DATAHUB"."ORACDC" ("ID", "NAME") VALUES (24, 'x');
    >
    >
    > //commit transation
    > COMMIT;
    >
    >
    > //rebuild dict.ora
    > EXECUTE dbms_logmnr_d.build(dictionary_filename => 'dict.ora', dictionary_location =>'/data/oracle/dict');
    > //set model redolog
    > EXECUTE DBMS_LOGMNR_D.BUILD(OPTIONS=> DBMS_LOGMNR_D.STORE_IN_REDO_LOGS);
    >
    >
    > //list log info
    > select group#, sequence#, bytes, members, status from v$log;
    > //add redolog file to logminer
    > EXEC dbms_logmnr.add_logfile('/data/oracle/oradata/ora11/redo01.log');
    > EXEC dbms_logmnr.add_logfile('/data/oracle/oradata/ora11/redo02.log');
    > EXEC dbms_logmnr.add_logfile('/data/oracle/oradata/ora11/redo03.log');
    > //add archivelog file to logminer
    > EXEC dbms_logmnr.add_logfile('/home/oracle/2018_01_15/o1_mf_1_5_f5rvv949_.arc');
    >
    > //execute logminer for analyzing log
    > EXECUTE DBMS_LOGMNR.START_LOGMNR(OPTIONS => DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG);
    >
    >
    > //1.execute logminer for analyzing log
    > EXEC dbms_logmnr.start_logmnr(OPTIONS => DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + DBMS_LOGMNR.CONTINUOUS_MINE + DBMS_LOGMNR.NO_SQL_DELIMITER);
    >
    >
    > //2.execute logminer for analyzing log
    > EXEC DBMS_LOGMNR.START_LOGMNR( STARTSCN => 989960, ENDSCN => 3004680, OPTIONS => DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + DBMS_LOGMNR.CONTINUOUS_MINE + DBMS_LOGMNR.NO_SQL_DELIMITER);
    >
    >
    > //3.execute logminer for analyzing log
    > EXEC DBMS_LOGMNR.START_LOGMNR( STARTTIME => to_date('2018-03-06 16:00:00','YYYY-MM-DD HH24:MI:SS'), ENDTIME => to_date('2018-03-06 16:40:00','YYYY-MM-DD HH24:MI:SS'), OPTIONS => DBMS_LOGMNR.DICT_FROM_ONLINE_CATALOG + DBMS_LOGMNR.CONTINUOUS_MINE + DBMS_LOGMNR.NO_SQL_DELIMITER);
    >
    >
    > //list result
    > select * from v$logmnr_contents;
    > select sql_redo,username,timestamp,sql_undo from v$logmnr_contents;
    >
    >
    > //list result by conditions
    > SELECT sql_redo FROM v$logmnr_contents WHERE username='datahub' AND table_name='ORACDC';
    > SELECT sql_redo FROM v$logmnr_contents WHERE table_name='ORACDC';
    >
    >
    > //close logminer 
    > EXEC dbms_logmnr.end_logmnr;
    >
    > 
    >
    > 
    >
    > 3、Oralce中归档内存空间管理
    >
    > connect datahub/datahub as sysdba
    > startup mount
    > select * from v$log;
    > select v1.group#, member, sequence#, first_change# from v$log v1, v$logfile v2 where v1.group# = v2.group#;
    > recover database until cancel;
    > recover database until time '2018-03-05 10:20:00';
    > alter database open resetlogs;
    >
    >
    > archive log list;
    > show parameter db_recover;
    >
    >
    > //查看归档区大小
    > select * from V$FLASH_RECOVERY_AREA_USAGE;
    >
    >
    > //修改恢复区大小
    > alter system set db_recovery_file_dest_size=1G;
    >
    >
    > //修改恢复区位置，默认有大小限制，自定义的地方无
    > alter system set log_archive_dest='location=/data/oracle/arch';
    >
    >
    > //删除前7天的归档文件,在rman中删除后，需要在对应路径下手动删除这些文件
    > rman>delete noprompt archivelog all completed before 'sysdate-7';
    > rman>delete noprompt archivelog until time 'sysdate - 7';
    >
    >
    > //切换redolog文件
    > alter system switch logfile;
    >
    >
    > //手动归档文件
    > alter system archive log current;
    >
    >
    > //手动清除无法归档的redolog文件
    > alter database clear unarchived logfile group 1;
    >
    > //设置归档文件的路径
    > alter system set log_archive_dest_1='location=/data/oracle/arch/archive_log' scope=both



## hana 及大部分关系型数据库

- trigger (触发器) 
  - （触发器（TRIGGER）是由事件来触发某个操作。这些事件包括INSERT语句、UPDATE语句和DELETE语句。当数据库系统执行这些事件时，会激活促发其执行相应的操作）
    - 通过trigger原理。我们可以在程序中捕获到表的insert、update、delete 事件。故可基于此我们可实现hana cdc方案。流程如下
      - 生成监听表的CDC日志表（id 自增）
      - 生成监听表的触发器
        - 触发器将捕获到更改记录，新增到CDC日志表中。
      - 在监控程序里，每隔一段时间循环查询CDC日志表，并通过id 进行增量获取。得到变更的数据
- 优缺点：
  - 优点：	
    - 触发器相对来说是个较通用的方案，可支持自定义的操作比较多。（编写trigger 过程）
  - 缺点
    - 侵入性比较大（需要在目标库中建立触发器及中间表）
    - DDL操作影响巨大。（当触发器执行失败时会导致造成原表的更新异常）



## PostgreSQL

- 与Mysql binlog stream实现方式。

- PostgreSQL在9.x之后引入了主从的流复制机制，所谓流复制，就是备服务器通过tcp流从主服务器中同步相应的数据，主服务器在WAL记录产生时即将它们以流式传送给备服务器，而不必等到WAL文件被填充。
- 默认情况下流复制是异步的，这种情况下主服务器上提交一个事务与该变化在备服务器上变得可见之间客观上存在短暂的延迟，但这种延迟相比基于文件的日志传送方式依然要小得多，在备服务器的能力满足负载的前提下延迟通常低于一秒。
  - 1、pgsql 的流复制
  - 2、wal2json 数据解析



## MongoDB

- 其中MongoReader基于MongoDB官方自带的[ChangeStream机制](https://docs.mongodb.com/manual/changeStreams/)。因此对MongoDB的部署有基本要求。

  MongoDB部署配置最低要求。

- https://docs.mongodb.com/manual/changeStreams/







## 总结

- 各个数据库之间的实时同步大都都依靠数据源其本身提供的主从复制进行实现。因为主库更新之后也需要通知从库保持数据的一致性。
- 但是不是每一个数据源都是开源的。就算是开源的时主库同步到从库方式也有可能不一一致。
  - 为了追求传输的字节紧缩小。可能都会用到字典进行处理。同步时主从状态要保持一致。或者他们对于操作方式也不一样
  - 比如jdbc 会用 sql。redis 会用 set命令。说白了就是命令之间的不同。转换之间也需要处理大量隐藏细节。

